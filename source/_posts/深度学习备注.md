---
title: 深度学习备注
date: 2018-11-22 11:30:05
tags:
---

Deep Learning!<!--more-->
## 激活函数
- Sigmod 二分类问题 Y是(0,1),适合使用Sigmod函数
- TanH 隐藏层推荐使用,值域为(-1,1),可以起到中心化的作用
- ReLU 因为TanH收敛太慢了,采用ReLu可以加快收敛,也是目前深度学习模型默认的激活函数



## 分类问题的损失函数:交叉熵(Cross Entropy Loss)
神经网络模型的效果及优化的目标是通过损失函数来定义的。

回归问题常用的损失函数是均方误差( MSE，mean squared error )。

分类问题常用的损失函数为交叉熵( Cross Entropy Loss)
交叉熵具体解释可以参考(https://blog.csdn.net/xg123321123/article/details/80781611)


## 
完全连接层(Fully Connected Layer)
局部连接层(Local Connected Layer)
池化层(Pooling Layer) 降低纬度,防止过拟合
图片增强 - 对训练集随机进行平移,旋转等操作
