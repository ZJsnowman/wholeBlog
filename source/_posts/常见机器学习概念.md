---
title: 常见数据挖掘术语解释
date: 2018-03-29 09:50:01
tags:
---


对一些常用的数据术语做一个解释<!--more-->

# 评判回归好坏的几个名词SSE,MSE,RMSE,R-square
SSE(误差平方和)：Sum Squared Error
MSE(均方误差)：Mean squared error
RMSE(均方根、标准差)：Root mean squared error
R-square(确定系数)：Coefficient of determination



一、SSE(误差平方和)  
该统计参数计算的是拟合数据和原始数据对应点的误差的平方和，计算公式如下
$$ SSE=\sum_{i=1}^{n}(y_{i}-f(x_{i}))^2 $$

SSE越接近于0，说明模型选择和拟合更好，数据预测也越成功。接下来的MSE和RMSE因为和SSE是同出一宗，所以效果一样




二、MSE(均方误差) Mean Squared Error
该统计参数是预测数据和原始数据对应点误差的平方和的均值，也就是SSE/n，和SSE没有太大的区别，计算公式如下
$$ MSE=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(x_{i}))^2 $$


三、RMSE(根均方误差)  Root Mean Squared Error 
该统计参数，也叫回归系统的拟合标准差，是MSE的平方根，就算公式如下
$$ RMSE=\sqrt{MSE}=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(x_{i}))^2} $$

**这里RMSE只是为了转换一下量纲,让其和y保持一致**

四. MAE 也是一个用来评判回归好坏的一个标准
MAE(Mean Absolute Error)
$$ MAE=\frac{1}{n}\sum_{i=1}^{n}\left | (y_{i}-f(x_{i}) \right | $$

这里会发现MAE和RMSE的值不一样,RMSE有放大预测结果和真是结果较大误差的趋势,因为RMSE内部有去平方的操作.误差大的会被平方操作放大.所以相比较MAE而言,优化RMSE更加具有意义.他会重点缩小预测值和真实值的较大误差.


在这之前，我们所有的误差参数都是基于预测值(y_hat)和原始值(y)之间的误差(即点对点)。从下面开始是所有的误差都是相对原始数据平均值(y_ba)而展开的(即点对全)!!!

四、R-square(决定系数)

RMSE或者MAE 也好.当我们做出一个模型,作用在预测房产数据是5万,预测分数是10分,这个时候无法判断这个模型的好坏.是预测房产价格好,还是预测分数好,因为这两个东西是不同种类的,所以这个时候就需要引入R-square.
在讲确定系数之前，我们需要介绍另外两个参数SSR和SST，因为确定系数就是由它们两个决定的
(1)SSR：Sum of squares of the regression，即预测数据与原始数据均值之差的平方和，公式如下
$$SSR=\sum_{i=1}^{n}(y_i-\bar{y})^2$$


(2)SST：Total sum of squares，即原始数据和均值之差的平方和，公式如下
$$SST=\sum_{i=1}^{n}(f_i-\bar{y})^2$$

细心的网友会发现，SST=SSE+SSR，呵呵只是一个有趣的问题。而我们的“确定系数”是定义为SSR和SST的比值，故
$$R-Square=\frac{SSR}{SST}$$

其实“确定系数”是通过数据的变化来表征一个拟合的好坏。由上面的表达式可以知道“确定系数”的正常取值范围为[0 1]，越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好


![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-12-142751.png)

![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-12-142919.png)

![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-12-143958.png)
# 超参数和模型参数
![](https://ws1.sinaimg.cn/large/006tKfTcgy1ftg4q7ypwxj30zw0pwq6b.jpg)


# 机器学习算法中距离和相似性度量方法
[参考链接](http://www.cnblogs.com/daniel-D/p/3244718.html)

# 网格搜索
网格搜索是 sklean 提供的一种方便进行超参数选择的方法.因为超参数的存在,所以我们最好的办法是遍历所有超参数的组合.但是很多时候不同的组合之间存在依赖关系,比如在 KNN 算法中,当`weights`为`uniform`的时候,是不需要对超参数 p(闵可夫斯基距离)进行遍历的.只有`weights`为`distance`的时候才需要遍历.所以 sklearn专门提供了一个方法来解决这个问题.

# 特征缩放
![](https://ws1.sinaimg.cn/large/006tKfTcgy1ftgh98tev8j312a0p20v9.jpg)
![](https://ws3.sinaimg.cn/large/006tKfTcgy1ftghamuas3j31bk0owdj0.jpg)


[归一化和标准化的区别](https://www.zhihu.com/question/20455227)


一般情况下优先标准化.在对输出有要求时再尝试别的方法，如归一化或者更加复杂的方法

## 最小二乘
这个最小二乘最初是由日本人翻译的.英语是`least square method`,中文也就这么稀里糊涂的借用过来了.简直无语,根本无法理解.直接看英语非常好理解.下次要在碰到这种无法理解的定义也要吸取教训直接看英语.另外值得一提的是绝对值那个叫最小一乘,英文叫`Least absolute deviations`
[最小二乘英文解释](https://en.wikipedia.org/wiki/Least_squares)
[最小一乘英文解释](https://en.wikipedia.org/wiki/Least_absolute_deviations)

## 梯度下降

![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-28-090644.png)
![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-28-082747.png)

![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-28-083109.png)


![](https://blog-image-1257302654.cos.ap-guangzhou.myqcloud.com/2018-08-29-084239.png)

为了加快运算速度才发明了随机梯度下降.当然也有坏处,具体可以参考下面的额lia啊


[随机梯度下降,批量梯度下降以及小批量梯度下降法对比](https://blog.csdn.net/u012328159/article/details/80252012)


**使用梯度下降前需要先对数据做一个归一化**