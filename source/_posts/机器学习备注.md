---
title: 机器学习备注
date: 2017-11-27 17:20:02
tags: [数据分析]
---
机器学习备注<!--more-->
# Naive Bayes(贝叶斯推断)
[讲解贝叶斯基本算法原理](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html)

## 朴素贝叶斯为啥朴素
因为贝叶斯算法并没有考虑事情发生的顺序.
![](https://ws1.sinaimg.cn/large/006tKfTcgy1fm10v16uz3j31c00s6n29.jpg)

## 朴素贝叶斯的优势和劣势
- 优势

这种算法非常适合文本分类。在处理文本时，常见的做法是将每个单词看作一个特征，这样就会有大量的特征。此算法的相对简单性和朴素贝叶斯独立特征的这一假设，使其能够出色完成文本的分类
- 劣势

当事情的顺序很重要的时候,就不行.比如早起 google芝加哥公牛的时候.就会出现一些芝加哥的介绍或者公牛的照片.但是明显芝加哥公牛有其他的意思.这个时候贝叶斯就不是很管用的

- 总结

如何避免这些,我们采用的监督式学习.也就有试验和测试数据.通过测试就能够知道算法是否有效


# SVM(支持向量机)
- 支持向量机的内部原理是最大程度提高结果稳健性
- 正确分类标签作为首要考虑,然后对间隔进行最大化.也就是说要在分类正确的前提下对间隔最大化
- SVM 可以忽略异常值,然后使间隔最大化

SVM 不适合处理海量数据,训练时间太慢了
噪声过多情况下,类严重重叠,边界不明心啊也不适合 SVM. 可以考虑用 Navie Bayes


  # Decision Trees (    )

## 信息熵
[信息熵的定义](http://www.ruanyifeng.com/blog/2014/09/information-entropy.html)
## 信息增益

## 优缺点
容易过拟合


# k-nearest neighbors
[查考](https://zhuanlan.zhihu.com/p/25994179)

# 集成学习
[参考](http://www.cnblogs.com/pinard/p/6131423.html)


# adaboost


# random forest


# 准确度与训练集大小的关系
更大的数据量要比经过精密调整的算法提供更好的结果,更大的数据能够取得更好的效果,这是
机器学习领域至理名言

# 数据类型
- numberical 数值
- categorical  分类
- time series  时序
- text  文字

# 判断离散还是连续
**关注顺序**

有些不好判断的类型,可以关注数据之间的顺序,如果之间有强烈的关联性,则用连续性的分类器,否则就用离散性的分类器.比如,公司100个人, 工号从1-100,如果需要给他们分类,最好就用离散性的分类器,因为不同工号之前没有关联性


# 涉及的一些中英对照
```
slope->斜率
intercept->截距
coefficient->系数
```


## 牛人总结
https://segmentfault.com/a/1190000012084849

## Kaggle 入门
https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA%3D%3D&mid=2650668868&idx=3&sn=ba389178ad651a78e35b3f16bb6ae560#wechat_redirect
